## 扩展卡尔曼滤波公式


1. **状态预测**：
   $$
   \hat{x}_{k|k-1} = f(\hat{x}_{k-1|k-1}, u_{k-1}) \tag{1}
   $$
   其中，$\hat{x}_{k|k-1}$ 是时刻 $k$ 的状态预测，$\hat{x}_{k-1|k-1}$ 是时刻 $k-1$ 的状态估计，$f$ 是状态转移函数，$u_{k-1}$ 是控制输入。

2. **协方差预测**：
   $$
   P_{k|k-1} = F_{k-1} P_{k-1|k-1} F_{k-1}^T + Q_{k-1} \tag{2}
   $$
   其中，$P_{k|k-1}$ 是时刻 $k$ 的协方差预测，$P_{k-1|k-1}$ 是时刻 $k-1$ 的协方差估计，$F_{k-1}$ 是状态转移函数 $f$ 对状态的雅可比矩阵，$Q_{k-1}$ 是过程噪声协方差。

### 测量更新

3. **卡尔曼增益**：
   $$
   K_k = P_{k|k-1} H_k^T (H_k P_{k|k-1} H_k^T + R_k)^{-1} \tag{3}
   $$
   其中，$K_k$ 是卡尔曼增益，$H_k$ 是观测函数 $h$ 对状态的雅可比矩阵，$R_k$ 是测量噪声协方差。

4. **状态更新**：
   $$
   \hat{x}_{k|k} = \hat{x}_{k|k-1} + K_k (z_k - h(\hat{x}_{k|k-1})) \tag{4}
   $$
   其中，$\hat{x}_{k|k}$ 是时刻 $k$ 的状态估计，$z_k$ 是时刻 $k$ 的测量值，$h$ 是观测函数。

5. **协方差更新**：
   $$
   P_{k|k} = (I - K_k H_k) P_{k|k-1} \tag{5}
   $$
   其中，$P_{k|k}$ 是时刻 $k$ 的协方差估计，$I$ 是单位矩阵。

### 雅可比矩阵的计算

- **状态转移函数的雅可比矩阵**：
  $$
  F_{k-1} = \left. \frac{\partial f}{\partial x} \right|_{x=\hat{x}_{k-1|k-1}, u=u_{k-1}} \tag{6}
  $$

- **观测函数的雅可比矩阵**：
  $$
  H_k = \left. \frac{\partial h}{\partial x} \right|_{x=\hat{x}_{k|k-1}} \tag{7}
  $$

这些公式定义了扩展卡尔曼滤波器的递推过程，通过将非线性系统模型线性化来估计系统的状态。

## 多类支持向量机损失 Multiclass Support Vector Machine Loss

损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值Delta。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。

让我们更精确一些。回忆一下，第i个数据中包含图像$x_i$的像素和代表正确类别的标签$y_i$。评分函数输入像素数据，然后通过公式 $f(x_i,W)$来计算不同分类类别的分值。这里我们将分值简写为s。比如，针对第j个类别的得分就是第j个元素：$s_{j}=f(x_{i},W)_{j}$。针对第i个数据的多类SVM的损失函数定义如下：

$$
L_i=\sum_{j\not=y_i}\max(0,s_j-s_{y_i}+\Delta) \tag{8}
$$

**举例**：用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值$s = [13,-7,11]$。其中第一个类别是正确类别，即$y_i=0$。同时假设$\Delta$是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j\not=y_i$）加起来，所以我们得到两个部分：  

$$
L_i=\max(0,-7-13+10)+\max(0,11-13+10) \tag{9}
$$

可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过$\max(0,-)$函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13>11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。

那么在这次的模型中，我们面对的是线性评分函数（ $f(x_i,W)=Wx_i$），所以我们可以将损失函数的公式稍微改写一下：

$$
L_i=\sum_{j\not=y_i}\max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta) \tag{10}
$$

其中$w_j$是权重$W$的第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数$f$公式，这样做就不是必须的了。

在结束这一小节前，还必须提一下的属于是关于0的阀值：$\max(0,-)$函数，它常被称为**折叶损失（hinge loss）**。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是$\max(0,-)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。

$$
\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert \tag{11}
$$
